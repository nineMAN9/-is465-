{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HWclassifydocuments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsPblEufFXT4CobgWkx6gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nineMAN9/mh429-is465-/blob/master/HWclassifydocuments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZshRGWoT83zB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e45ac7bb-e10b-496b-981d-3773cc595250"
      },
      "source": [
        "print(\"hello world\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPRBSTFLGU2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c1563cdc-3adc-45ce-b859-e62678200144"
      },
      "source": [
        "      print(  \"\\n\",\n",
        "        \"# Author: Olivier Grisel <olivier.grisel@ensta.org>\\n\",\n",
        "        \"#         Lars Buitinck\\n\",\n",
        "        \"#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\\n\",\n",
        "        \"# License: BSD 3 clause\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " # Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
            " #         Lars Buitinck\n",
            " #         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
            " # License: BSD 3 clause\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLmxTryFGG63",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KjPg8rOGF12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a9b494e9-d071-415a-f16e-7f87f70aafc2"
      },
      "source": [
        "print(\n",
        "\"import some packages: python needs time to understand \\n\",\n",
        "        \"web mining is mining the text\\n\",\n",
        "        \"web mining can be great\\n\",\n",
        "        \"lets learn together\\n\",\n",
        "        \"idf log3/2\\n\",\n",
        "        \"tf-idf 1/6*log3/2\\n\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import some packages: python needs time to understand \n",
            " web mining is mining the text\n",
            " web mining can be great\n",
            " lets learn together\n",
            " idf log3/2\n",
            " tf-idf 1/6*log3/2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q7IQzJIHXWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2f840bb7-c8f0-4108-d71c-6964635e81c0"
      },
      "source": [
        "print( \"from time import time\\n\",\n",
        "        \"\\n\",\n",
        "        \"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\\n\",\n",
        "        \"from sklearn.decomposition import NMF, LatentDirichletAllocation\\n\",\n",
        "        \"from sklearn.datasets import fetch_20newsgroups\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from time import time\n",
            " \n",
            " from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
            " from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
            " from sklearn.datasets import fetch_20newsgroups\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghQg9NxAHgzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0dbe71db-cefd-4ee9-ff9e-cdcfd8883559"
      },
      "source": [
        "print(\"Define some variables\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Define some variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUo-V65MHkIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "86d5f73f-9f77-4fdd-86da-2fbf31da7032"
      },
      "source": [
        "        print(\"n_samples = 2000\\n\",\n",
        "        \"n_features = 1000\\n\",\n",
        "        \"n_components = 10\\n\",\n",
        "        \"n_top_words = 20\\n\",\n",
        "        \"print(n_samples)\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_samples = 2000\n",
            " n_features = 1000\n",
            " n_components = 10\n",
            " n_top_words = 20\n",
            " print(n_samples)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxC3HGKjHym4",
        "colab_type": "text"
      },
      "source": [
        "Define function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T-g__h0HvdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f6fcb6b3-1d12-4362-e949-f61375ac1680"
      },
      "source": [
        "print(\"def print_top_words(model, feature_names, n_top_words):\\n\",\n",
        "        \"    for topic_idx, topic in enumerate(model.components_):\\n\",\n",
        "        \"        message = \\\"Topic #%d: \\\" % topic_idx\\n\",\n",
        "        \"        message += \\\" \\\".join([feature_names[i]\\n\",\n",
        "        \"                             for i in topic.argsort()[:-n_top_words - 1:-1]])\\n\",\n",
        "        \"        print(message)\\n\",\n",
        "        \"    print()\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def print_top_words(model, feature_names, n_top_words):\n",
            "     for topic_idx, topic in enumerate(model.components_):\n",
            "         message = \"Topic #%d: \" % topic_idx\n",
            "         message += \" \".join([feature_names[i]\n",
            "                              for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
            "         print(message)\n",
            "     print()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqrLqo9OIDG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4e940925-58ee-4284-b105-f08534132693"
      },
      "source": [
        "print(\"# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\\n\",\n",
        "        \"# to filter out useless terms early on: the posts are stripped of headers,\\n\",\n",
        "        \"# footers and quoted replies, and common English words, words occurring in\\n\",\n",
        "        \"# only one document or in at least 95% of the documents are removed.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
            " # to filter out useless terms early on: the posts are stripped of headers,\n",
            " # footers and quoted replies, and common English words, words occurring in\n",
            " # only one document or in at least 95% of the documents are removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ML9LIdIIDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c539d96d-3ab3-4338-f063-311c1015e086"
      },
      "source": [
        "    print(   \"print(\\\"Loading dataset...\\\")\\n\",\n",
        "        \"t0 = time()\\n\",\n",
        "        \"data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\\n\",\n",
        "        \"                             remove=('headers', 'footers', 'quotes'),\\n\",\n",
        "        \"                             return_X_y=True)\\n\",\n",
        "        \"data_samples = data[:n_samples]\\n\",\n",
        "        \"print(\\\"done in %0.3fs.\\\" % (time() - t0))\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print(\"Loading dataset...\")\n",
            " t0 = time()\n",
            " data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
            "                              remove=('headers', 'footers', 'quotes'),\n",
            "                              return_X_y=True)\n",
            " data_samples = data[:n_samples]\n",
            " print(\"done in %0.3fs.\" % (time() - t0))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA7VGGepIK08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b28126dc-0c18-438f-abc0-9483db21127e"
      },
      "source": [
        "print(        \"print(\\\"Extracting tf-idf features for NMF...\\\")\\n\",\n",
        "        \"tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\\n\",\n",
        "        \"                                   max_features=n_features,\\n\",\n",
        "        \"                                   stop_words='english')\\n\",\n",
        "        \"t0 = time()\\n\",\n",
        "        \"tfidf = tfidf_vectorizer.fit_transform(data_samples)\\n\",\n",
        "        \"print(\\\"done in %0.3fs.\\\" % (time() - t0))\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print(\"Extracting tf-idf features for NMF...\")\n",
            " tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
            "                                    max_features=n_features,\n",
            "                                    stop_words='english')\n",
            " t0 = time()\n",
            " tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
            " print(\"done in %0.3fs.\" % (time() - t0))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGpj_mFwISCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "958345cc-63ed-426f-e255-70de4d86c097"
      },
      "source": [
        "     print(   \"print(\\\"Extracting tf features for LDA...\\\")\\n\",\n",
        "        \"tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\\n\",\n",
        "        \"                                max_features=n_features,\\n\",\n",
        "        \"                                stop_words='english')\\n\",\n",
        "        \"t0 = time()\\n\",\n",
        "        \"tf = tf_vectorizer.fit_transform(data_samples)\\n\",\n",
        "        \"print(\\\"done in %0.3fs.\\\" % (time() - t0))\\n\",\n",
        "        \"print()\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print(\"Extracting tf features for LDA...\")\n",
            " tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
            "                                 max_features=n_features,\n",
            "                                 stop_words='english')\n",
            " t0 = time()\n",
            " tf = tf_vectorizer.fit_transform(data_samples)\n",
            " print(\"done in %0.3fs.\" % (time() - t0))\n",
            " print()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krnIpxzmIl-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "748111ec-d9f1-4850-ef1a-4622d4b28b09"
      },
      "source": [
        "print(  \"print(\\\"Fitting the NMF model (Frobenius norm) with tf-idf features, \\\"\\n\",\n",
        "        \"      \\\"n_samples=%d and n_features=%d...\\\"\\n\",\n",
        "        \"      % (n_samples, n_features))\\n\",\n",
        "        \"t0 = time()\\n\",\n",
        "        \"nmf = NMF(n_components=n_components, random_state=1,\\n\",\n",
        "        \"          alpha=.1, l1_ratio=.5).fit(tfidf)\\n\",\n",
        "        \"print(\\\"done in %0.3fs.\\\" % (time() - t0))\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nTopics in NMF model (Frobenius norm):\\\")\\n\",\n",
        "        \"tfidf_feature_names = tfidf_vectorizer.get_feature_names()\\n\",\n",
        "        \"print_top_words(nmf, tfidf_feature_names, n_top_words)\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
            "       \"n_samples=%d and n_features=%d...\"\n",
            "       % (n_samples, n_features))\n",
            " t0 = time()\n",
            " nmf = NMF(n_components=n_components, random_state=1,\n",
            "           alpha=.1, l1_ratio=.5).fit(tfidf)\n",
            " print(\"done in %0.3fs.\" % (time() - t0))\n",
            " \n",
            " print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
            " tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
            " print_top_words(nmf, tfidf_feature_names, n_top_words)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq-wSgwgIvvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "439d72d1-fc39-4c4b-ce53-da2c04ed8d52"
      },
      "source": [
        "print( \"Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\\n\",\n",
        "            \"done in 0.379s.\\n\",\n",
        "            \"\\n\",\n",
        "            \"Topics in NMF model (Frobenius norm):\\n\",\n",
        "            \"Topic #0: just people don think like know time good make way really say right ve want did ll new use years\\n\",\n",
        "            \"Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\\n\",\n",
        "            \"Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\\n\",\n",
        "            \"Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\\n\",\n",
        "            \"Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\\n\",\n",
        "            \"Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\\n\",\n",
        "            \"Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\\n\",\n",
        "            \"Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\\n\",\n",
        "            \"Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\\n\",\n",
        "            \"Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\\n\",)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
            " done in 0.379s.\n",
            " \n",
            " Topics in NMF model (Frobenius norm):\n",
            " Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
            " Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
            " Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
            " Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
            " Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
            " Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
            " Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
            " Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
            " Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
            " Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQSbU6zBI2px",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "c3721c71-36a1-4439-c879-41824ab1492a"
      },
      "source": [
        "print(\n",
        "        \"print(\\\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \\\"\\n\",\n",
        "        \"      \\\"tf-idf features, n_samples=%d and n_features=%d...\\\"\\n\",\n",
        "        \"      % (n_samples, n_features))\\n\",\n",
        "        \"t0 = time()\\n\",\n",
        "        \"nmf = NMF(n_components=n_components, random_state=1,\\n\",\n",
        "        \"          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\\n\",\n",
        "        \"          l1_ratio=.5).fit(tfidf)\\n\",\n",
        "        \"print(\\\"done in %0.3fs.\\\" % (time() - t0))\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nTopics in NMF model (generalized Kullback-Leibler divergence):\\\")\\n\",\n",
        "        \"tfidf_feature_names = tfidf_vectorizer.get_feature_names()\\n\",\n",
        "        \"print_top_words(nmf, tfidf_feature_names, n_top_words)\\n\",)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
            "       \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
            "       % (n_samples, n_features))\n",
            " t0 = time()\n",
            " nmf = NMF(n_components=n_components, random_state=1,\n",
            "           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
            "           l1_ratio=.5).fit(tfidf)\n",
            " print(\"done in %0.3fs.\" % (time() - t0))\n",
            " \n",
            " print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
            " tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
            " print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Fd6QHnJAyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "d3c883a9-c5a0-430b-87c7-1a12ebf7313f"
      },
      "source": [
        "print(          \n",
        "            \"Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\\n\",\n",
        "            \"done in 1.507s.\\n\",\n",
        "            \"\\n\",\n",
        "            \"Topics in NMF model (generalized Kullback-Leibler divergence):\\n\",\n",
        "            \"Topic #0: people don just like think did say time make know really right said things way ve course didn question probably\\n\",\n",
        "            \"Topic #1: windows help thanks using hi looking info video dos pc does anybody ftp appreciated mail know advance available use card\\n\",\n",
        "            \"Topic #2: god does jesus true book christian bible christians religion faith believe life church christ says know read exist lord people\\n\",\n",
        "            \"Topic #3: thanks know bike interested mail like new car edu heard just price list email hear want cars thing sounds reply\\n\",\n",
        "            \"Topic #4: 10 00 sale time power 12 new 15 year 30 offer condition 14 16 model 11 monitor 100 old 25\\n\",\n",
        "            \"Topic #5: space government number public data states earth security water research nasa general 1993 phone information science technology provide blood internet\\n\",\n",
        "            \"Topic #6: edu file com program soon try window problem remember files sun send library article mike wrong think code win manager\\n\",\n",
        "            \"Topic #7: game team year games play win season points world division won players nhl flyers toronto case cubs teams ll record\\n\",\n",
        "            \"Topic #8: drive think hard software disk drives apple computer mac need scsi card don problem read floppy post cable going ii\\n\",\n",
        "            \"Topic #9: use good just key chip got like ll way clipper doesn keys don better speed stuff want sure going need\\n\",\n",
        "            \"\\n\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
            " done in 1.507s.\n",
            " \n",
            " Topics in NMF model (generalized Kullback-Leibler divergence):\n",
            " Topic #0: people don just like think did say time make know really right said things way ve course didn question probably\n",
            " Topic #1: windows help thanks using hi looking info video dos pc does anybody ftp appreciated mail know advance available use card\n",
            " Topic #2: god does jesus true book christian bible christians religion faith believe life church christ says know read exist lord people\n",
            " Topic #3: thanks know bike interested mail like new car edu heard just price list email hear want cars thing sounds reply\n",
            " Topic #4: 10 00 sale time power 12 new 15 year 30 offer condition 14 16 model 11 monitor 100 old 25\n",
            " Topic #5: space government number public data states earth security water research nasa general 1993 phone information science technology provide blood internet\n",
            " Topic #6: edu file com program soon try window problem remember files sun send library article mike wrong think code win manager\n",
            " Topic #7: game team year games play win season points world division won players nhl flyers toronto case cubs teams ll record\n",
            " Topic #8: drive think hard software disk drives apple computer mac need scsi card don problem read floppy post cable going ii\n",
            " Topic #9: use good just key chip got like ll way clipper doesn keys don better speed stuff want sure going need\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obFO6kwCJEWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "8efa7495-dd69-4274-b7e3-0b65f251115f"
      },
      "source": [
        "print(     \"print(\\\"Fitting LDA models with tf features, \\\"\\n\",\n",
        "        \"      \\\"n_samples=%d and n_features=%d...\\\"\\n\",\n",
        "        \"      % (n_samples, n_features))\\n\",\n",
        "        \"lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\\n\",\n",
        "        \"                                learning_method='online',\\n\",\n",
        "        \"                                learning_offset=50.,\\n\",\n",
        "        \"                                random_state=0)\\n\",\n",
        "        \"t0 = time()\\n\",\n",
        "        \"lda.fit(tf)\\n\",\n",
        "        \"print(\\\"done in %0.3fs.\\\" % (time() - t0))\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nTopics in LDA model:\\\")\\n\",\n",
        "        \"tf_feature_names = tf_vectorizer.get_feature_names()\\n\",\n",
        "        \"print_top_words(lda, tf_feature_names, n_top_words)\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "print(\"Fitting LDA models with tf features, \"\n",
            "       \"n_samples=%d and n_features=%d...\"\n",
            "       % (n_samples, n_features))\n",
            " lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
            "                                 learning_method='online',\n",
            "                                 learning_offset=50.,\n",
            "                                 random_state=0)\n",
            " t0 = time()\n",
            " lda.fit(tf)\n",
            " print(\"done in %0.3fs.\" % (time() - t0))\n",
            " \n",
            " print(\"\\nTopics in LDA model:\")\n",
            " tf_feature_names = tf_vectorizer.get_feature_names()\n",
            " print_top_words(lda, tf_feature_names, n_top_words)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}